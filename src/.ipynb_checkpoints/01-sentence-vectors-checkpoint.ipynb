{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sentence Vectors from Word Vectors\n",
    "\n",
    "Ideally, would be preferable to train a network end-to-end to predict document similarity, but we need to start with word vectors to generate sentence vectors, and generate document vectors from sentence vectors. Because we will generate sentence vectos using different inputs (words in a sentence) than document vectors (sentences in a document), we will build an autoencoder to generate our sentence vectors below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import RepeatVector\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "\n",
    "VOCAB_FILE = os.path.join(DATA_DIR, \"rt-vocab.tsv\")\n",
    "SENT_FILE = os.path.join(DATA_DIR, \"rt-sent.tsv\")\n",
    "\n",
    "GLOVE_FILE = os.path.join(DATA_DIR, \"glove.840B.300d.txt\")\n",
    "WORD_EMBED_SIZE = 300\n",
    "\n",
    "MAX_SEQLEN = 50\n",
    "SENT_EMBED_SIZE = 512\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "MODEL_FILE = os.path.join(DATA_DIR, \"sent-encoder.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Vocabulary for word lookup\n",
    "\n",
    "We replace any word that occurs 5 times or less across the corpus is replaced by the pseudo word \\_UNK\\_. The \\_PAD\\_ character is meant to padd short sentences with a standard character (0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2word = {}\n",
    "id2word = {0: \"_PAD_\", 1: \"_UNK_\"}\n",
    "fvoc = open(VOCAB_FILE, \"rb\")\n",
    "for i, line in enumerate(fvoc):\n",
    "    word, count = line.strip().split(\"\\t\")\n",
    "    count = int(count)\n",
    "    if count <= 5:\n",
    "        continue\n",
    "    id2word[i+2] = word\n",
    "fvoc.close()\n",
    "word2id = {v:k for k,v in id2word.items()}\n",
    "vocab_size = len(word2id)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract embeddings from GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "E = np.zeros((len(word2id), GLOVE_EMBED_SIZE))\n",
    "# UNK is given a random value\n",
    "E[1] = np.random.random((GLOVE_EMBED_SIZE))\n",
    "fglo = open(GLOVE_FILE, \"rb\")\n",
    "for line in fglo:\n",
    "    cols = line.strip().split(\" \")\n",
    "    word = cols[0]\n",
    "    vec = [float(x) for x in cols[1:]]\n",
    "    try:\n",
    "        i = word2id[word]\n",
    "        E[i] = vec\n",
    "    except KeyError:\n",
    "        # word does not exist in vocab\n",
    "        continue\n",
    "fglo.close()\n",
    "print(E.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentences as word id sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xdata = []\n",
    "fsent = open(SENT_FILE, \"rb\")\n",
    "sent_id = 0\n",
    "for line in fsent:\n",
    "    _, _, sent = line.strip().split(\"\\t\")\n",
    "    word_ids = []\n",
    "    for word in nltk.word_tokenize(sent):\n",
    "        try:\n",
    "            word_id = word2id[word]\n",
    "        except KeyError:\n",
    "            word_id = word2id[\"_UNK_\"]\n",
    "        word_ids.append(word_id)\n",
    "    xdata.append(np.array(word_ids))\n",
    "fsent.close()\n",
    "X = pad_sequences(np.array(xdata), MAX_SEQLEN)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest = train_test_split(X, train_size=0.9)\n",
    "print(Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def datagen(X, E, batch_size=BATCH_SIZE):\n",
    "    while True:\n",
    "        # loop once per epoch\n",
    "        num_recs = X.shape[0]\n",
    "        indices = np.random.permutation(np.arange(num_recs))\n",
    "        num_batches = num_recs // batch_size\n",
    "        for bid in range(num_batches):\n",
    "            sids = indices[bid * batch_size : (bid + 1) * batch_size]\n",
    "            Xbatch = E[X[sids, :]]\n",
    "            yield Xbatch, Xbatch\n",
    "            \n",
    "train_gen = datagen(Xtrain, E)\n",
    "Xb, Xb = train_gen.next()\n",
    "print(Xtrain.shape, Xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(MAX_SEQLEN, WORD_EMBED_SIZE))\n",
    "encoded = Bidirectional(LSTM(SENT_EMBED_SIZE), merge_mode=\"sum\",\n",
    "                       name=\"encoder_lstm\")(inputs)\n",
    "decoded = RepeatVector(MAX_SEQLEN)(encoded)\n",
    "outputs = Bidirectional(LSTM(WORD_EMBED_SIZE, return_sequences=True),\n",
    "                        merge_mode=\"sum\")(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=\"adam\", loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for layer in autoencoder.layers:\n",
    "    print(layer.name, layer.input_shape, layer.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train_steps = len(Xtrain) // BATCH_SIZE\n",
    "num_test_steps = len(Xtest) // BATCH_SIZE\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=MODEL_FILE, save_best_only=True)\n",
    "\n",
    "train_gen = datagen(Xtrain, E)\n",
    "test_gen = datagen(Xtest, E)\n",
    "\n",
    "history = autoencoder.fit_generator(train_gen, \n",
    "                                    steps_per_epoch=num_train_steps,\n",
    "                                    epochs=NUM_EPOCHS,\n",
    "                                    validation_data=test_gen,\n",
    "                                    validation_steps=num_test_steps,\n",
    "                                    callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Encoder Portion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder = Model(autoencoder.input, \n",
    "                autoencoder.get_layer(\"encoder_lstm\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
